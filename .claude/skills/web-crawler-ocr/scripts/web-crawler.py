#!/usr/bin/env python3
"""
ë²”ìš© ì›¹ í¬ë¡¤ëŸ¬ + Gemini OCR í†µí•© ì‹œìŠ¤í…œ

Firecrawlë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ â†’ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ â†’ Gemini OCR â†’ ì™„ì „í•œ ë§ˆí¬ë‹¤ìš´ ìƒì„±

ì‚¬ìš©ë²•:
  python web-crawler.py <URL> [ì¶œë ¥_íŒŒì¼ëª…]

ì˜ˆì‹œ:
  python web-crawler.py https://example.com/article
  python web-crawler.py https://competitor-cafe.com analysis.md
"""

import os
import sys
import json
import requests
import base64
from pathlib import Path
from datetime import datetime
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import google.generativeai as genai

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
FIRECRAWL_API_KEY = os.getenv("FIRECRAWL_API_KEY")

# ìƒ‰ìƒ ì¶œë ¥
class Colors:
    BLUE = '\033[0;34m'
    GREEN = '\033[0;32m'
    YELLOW = '\033[1;33m'
    RED = '\033[0;31m'
    NC = '\033[0m'

def print_color(text, color):
    """ìƒ‰ìƒ ì¶œë ¥"""
    print(f"{color}{text}{Colors.NC}")

def check_api_keys():
    """API í‚¤ í™•ì¸"""
    missing = []

    if not GEMINI_API_KEY:
        missing.append("GEMINI_API_KEY")

    if not FIRECRAWL_API_KEY:
        missing.append("FIRECRAWL_API_KEY")

    if missing:
        print_color(f"âŒ ë‹¤ìŒ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {', '.join(missing)}", Colors.RED)
        print("\nì„¤ì • ë°©ë²•:")
        print("1. .env íŒŒì¼ ìƒì„± ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ ì„¤ì •:")
        print("   GEMINI_API_KEY='your_key_here'")
        print("   FIRECRAWL_API_KEY='your_key_here'")
        print("\n2. API í‚¤ ë°œê¸‰:")
        print("   - Gemini: https://aistudio.google.com/apikey")
        print("   - Firecrawl: https://firecrawl.dev")
        sys.exit(1)

def crawl_with_firecrawl(url):
    """Firecrawlë¡œ ì›¹í˜ì´ì§€ í¬ë¡¤ë§"""
    print_color(f"ğŸ” Firecrawlë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘: {url}", Colors.BLUE)

    try:
        from firecrawl import FirecrawlApp

        app = FirecrawlApp(api_key=FIRECRAWL_API_KEY)
        result = app.scrape_url(url, params={
            'formats': ['markdown', 'html'],
            'onlyMainContent': True
        })

        if result.get('success'):
            markdown = result.get('markdown', '')
            html = result.get('html', '')
            metadata = result.get('metadata', {})

            print_color(f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(markdown)} ê¸€ì)", Colors.GREEN)
            return {
                'markdown': markdown,
                'html': html,
                'metadata': metadata
            }
        else:
            print_color("âš ï¸ Firecrawl ì‹¤íŒ¨, BeautifulSoupìœ¼ë¡œ ëŒ€ì²´", Colors.YELLOW)
            return crawl_with_beautifulsoup(url)

    except ImportError:
        print_color("âš ï¸ firecrawl-py ë¯¸ì„¤ì¹˜, BeautifulSoupìœ¼ë¡œ ëŒ€ì²´", Colors.YELLOW)
        return crawl_with_beautifulsoup(url)
    except Exception as e:
        print_color(f"âš ï¸ Firecrawl ì˜¤ë¥˜: {e}, BeautifulSoupìœ¼ë¡œ ëŒ€ì²´", Colors.YELLOW)
        return crawl_with_beautifulsoup(url)

def crawl_with_beautifulsoup(url):
    """BeautifulSoupìœ¼ë¡œ ëŒ€ì²´ í¬ë¡¤ë§"""
    print_color("ğŸ”„ BeautifulSoupìœ¼ë¡œ í¬ë¡¤ë§ ì¤‘...", Colors.BLUE)

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    response = requests.get(url, headers=headers, timeout=30)
    response.raise_for_status()

    soup = BeautifulSoup(response.content, 'html.parser')

    # ë³¸ë¬¸ ì¶”ì¶œ
    article = soup.find('article') or soup.find('main') or soup.body
    text = article.get_text(separator='\n', strip=True) if article else soup.get_text(separator='\n', strip=True)

    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    title = soup.find('title')
    title_text = title.text if title else urlparse(url).netloc

    return {
        'markdown': text,
        'html': str(soup),
        'metadata': {
            'title': title_text,
            'sourceURL': url
        }
    }

def extract_images_from_html(html, base_url):
    """HTMLì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
    soup = BeautifulSoup(html, 'html.parser')
    images = []

    for img in soup.find_all('img'):
        src = img.get('src') or img.get('data-src')
        if src:
            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            full_url = urljoin(base_url, src)

            # alt í…ìŠ¤íŠ¸
            alt = img.get('alt', '')

            images.append({
                'url': full_url,
                'alt': alt
            })

    print_color(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ {len(images)}ê°œ ë°œê²¬", Colors.GREEN)
    return images

def download_image(url, output_dir):
    """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()

        # íŒŒì¼ëª… ìƒì„±
        filename = Path(urlparse(url).path).name or 'image.jpg'
        filepath = output_dir / filename

        with open(filepath, 'wb') as f:
            f.write(response.content)

        return filepath
    except Exception as e:
        print_color(f"âš ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url} ({e})", Colors.YELLOW)
        return None

def analyze_image_with_gemini(image_path):
    """Geminië¡œ ì´ë¯¸ì§€ OCR ë° ë¶„ì„"""
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-2.5-flash')

        # ì´ë¯¸ì§€ ë¡œë“œ
        with open(image_path, 'rb') as f:
            image_data = f.read()

        # Geminiì— ë¶„ì„ ìš”ì²­
        response = model.generate_content([
            "ì´ ì´ë¯¸ì§€ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³ , ì£¼ìš” ë‚´ìš©ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”. í•œê¸€ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
            {'mime_type': 'image/jpeg', 'data': image_data}
        ])

        return response.text
    except Exception as e:
        print_color(f"âš ï¸ Gemini ë¶„ì„ ì‹¤íŒ¨: {e}", Colors.YELLOW)
        return None

def process_url(url, output_file=None):
    """URL ì „ì²´ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°"""

    print_color(f"\n{'='*80}", Colors.BLUE)
    print_color(f"ğŸš€ ì›¹ í¬ë¡¤ë§ + OCR ì‹œì‘", Colors.BLUE)
    print_color(f"{'='*80}\n", Colors.BLUE)

    # 1. Firecrawlë¡œ í…ìŠ¤íŠ¸ í¬ë¡¤ë§
    crawl_result = crawl_with_firecrawl(url)
    markdown_text = crawl_result['markdown']
    html = crawl_result['html']
    metadata = crawl_result['metadata']

    # 2. ì´ë¯¸ì§€ ì¶”ì¶œ
    images = extract_images_from_html(html, url)

    # 3. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° OCR
    temp_dir = Path('/tmp/web-crawler-images')
    temp_dir.mkdir(exist_ok=True)

    image_analyses = []

    if images:
        print_color(f"\nğŸ¤– Gemini OCR ì²˜ë¦¬ ì¤‘ ({len(images)}ê°œ ì´ë¯¸ì§€)...", Colors.BLUE)

        for i, img in enumerate(images[:10], 1):  # ìµœëŒ€ 10ê°œê¹Œì§€
            print(f"  [{i}/{min(len(images), 10)}] {img['url']}")

            # ë‹¤ìš´ë¡œë“œ
            filepath = download_image(img['url'], temp_dir)
            if not filepath:
                continue

            # Gemini OCR
            analysis = analyze_image_with_gemini(filepath)
            if analysis:
                image_analyses.append({
                    'url': img['url'],
                    'alt': img['alt'],
                    'analysis': analysis
                })

    # 4. ìµœì¢… ë§ˆí¬ë‹¤ìš´ ìƒì„±
    final_markdown = generate_final_markdown(
        url=url,
        metadata=metadata,
        text=markdown_text,
        image_analyses=image_analyses
    )

    # 5. íŒŒì¼ ì €ì¥
    if not output_file:
        # URLì—ì„œ íŒŒì¼ëª… ìƒì„±
        domain = urlparse(url).netloc.replace('www.', '')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f"{domain}_{timestamp}.md"

    output_path = Path(output_file)
    output_path.write_text(final_markdown, encoding='utf-8')

    print_color(f"\n{'='*80}", Colors.GREEN)
    print_color(f"âœ… ì™„ë£Œ: {output_path.absolute()}", Colors.GREEN)
    print_color(f"{'='*80}\n", Colors.GREEN)

    return output_path

def generate_final_markdown(url, metadata, text, image_analyses):
    """ìµœì¢… ë§ˆí¬ë‹¤ìš´ ìƒì„±"""

    title = metadata.get('title', 'Untitled')
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    md = f"""# {title}

> ì¶œì²˜: {url}
> í¬ë¡¤ë§: {timestamp}
> ë„êµ¬: Firecrawl + Gemini OCR

---

## ğŸ“„ í…ìŠ¤íŠ¸ ë‚´ìš©

{text}

"""

    if image_analyses:
        md += "\n---\n\n## ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„ (Gemini OCR)\n\n"

        for i, img in enumerate(image_analyses, 1):
            md += f"### ì´ë¯¸ì§€ {i}\n\n"
            md += f"- **URL**: {img['url']}\n"
            if img['alt']:
                md += f"- **Alt í…ìŠ¤íŠ¸**: {img['alt']}\n"
            md += f"\n**ë¶„ì„ ê²°ê³¼:**\n\n{img['analysis']}\n\n---\n\n"

    return md

def main():
    """ë©”ì¸ ì‹¤í–‰"""

    # API í‚¤ í™•ì¸
    check_api_keys()

    # ì¸ì í™•ì¸
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python web-crawler.py <URL> [ì¶œë ¥_íŒŒì¼ëª…]")
        print("\nì˜ˆì‹œ:")
        print("  python web-crawler.py https://example.com/article")
        print("  python web-crawler.py https://competitor.com analysis.md")
        sys.exit(1)

    url = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) > 2 else None

    # ì²˜ë¦¬ ì‹¤í–‰
    result = process_url(url, output_file)

    print(f"ğŸ“‚ ê²°ê³¼ íŒŒì¼: {result}")
    print("\nğŸ’¡ Claude Codeì—ì„œ í™œìš©:")
    print(f"   > {result} ì½ê³  í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì •ë¦¬í•´ì¤˜")

if __name__ == "__main__":
    main()
